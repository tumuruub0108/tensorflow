{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c4d148e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-26 14:20:44.312316: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-10-26 14:20:44.312545: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-10-26 14:20:44.347515: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-10-26 14:20:45.107463: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-10-26 14:20:45.107701: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da9a5c2",
   "metadata": {},
   "source": [
    "# Building Complex Models Using the Functional API\n",
    "One example of a nonsequential neural network is a Wide & Deep neural network\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/sambitmukherjee/handson-ml3-pytorch/main/chapter10/Figure_10-13.png\" width=\"480\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8fb9f3ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing = fetch_california_housing()\n",
    " \n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(housing.data, housing.target, random_state=42)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train_full, y_train_full, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c3c3a6f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalization_layer = tf.keras.layers.Normalization()\n",
    "hidden_layer1 = tf.keras.layers.Dense(30, activation=\"relu\")\n",
    "hidden_layer2 = tf.keras.layers.Dense(30, activation=\"relu\")\n",
    "concat_layer = tf.keras.layers.Concatenate()\n",
    "output_layer = tf.keras.layers.Dense(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b661fb59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-26 14:20:45.865908: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n"
     ]
    }
   ],
   "source": [
    "input_ = tf.keras.layers.Input(shape=X_train.shape[1:])\n",
    "normalized = normalization_layer(input_)\n",
    "hidden1 = hidden_layer1(normalized)\n",
    "hidden2 = hidden_layer2(hidden1)\n",
    "concat = concat_layer([normalized, hidden2])\n",
    "output = output_layer(concat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b1f14eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Model(inputs=[input_], outputs=[output])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a2995a",
   "metadata": {},
   "source": [
    "But what if you want to send a subset of the features through the wide path and a different subset (possibly overlapping) through the deep path, In this case, one solution is to use multiple inputs. For example, suppose we want to send five features through the wide path (features 0 to 4), and six features through the deep path (features 2 to 7). We can do this as follows:\n",
    "\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/sambitmukherjee/handson-ml3-pytorch/main/chapter10/Figure_10-14.png\" width=\"480\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a94eba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_wide = tf.keras.layers.Input(shape=[5]) # features 0 to 4\n",
    "input_deep = tf.keras.layers.Input(shape=[6]) # features 2 to 7\n",
    "\n",
    "norm_layer_wide = tf.keras.layers.Normalization()\n",
    "norm_layer_deep = tf.keras.layers.Normalization()\n",
    "\n",
    "norm_wide = norm_layer_wide(input_wide)\n",
    "norm_deep = norm_layer_deep(input_deep)\n",
    "\n",
    "hidden1 = tf.keras.layers.Dense(30, activation=\"relu\")(norm_deep)\n",
    "hidden2 = tf.keras.layers.Dense(30, activation=\"relu\")(hidden1)\n",
    "\n",
    "concat = tf.keras.layers.concatenate([norm_wide, hidden2])\n",
    "output = tf.keras.layers.Dense(1)(concat)\n",
    "\n",
    "model = tf.keras.Model(inputs=[input_wide, input_deep], outputs=[output])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37608abd",
   "metadata": {},
   "source": [
    "There are a few things to note in this example, compared to the previous one:\n",
    "\n",
    "Each Dense layer is created and called on the same line. This is a common practice, as it makes the code more concise without losing clarity. However, we can’t do this with the Normalization layer since we need a reference to the layer to be able to call its adapt() method before fitting the model.\n",
    "\n",
    "\n",
    "\n",
    "We used tf.keras.layers.concatenate(), which creates a Concatenate layer and calls it with the given inputs.\n",
    "\n",
    "\n",
    "We specified inputs=[input_wide, input_deep] when creating the model, since there are two inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "185d1f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "model.compile(loss=\"mse\", optimizer=optimizer, metrics=[\"RootMeanSquaredError\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a58ec9bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_wide, X_train_deep = X_train[:, :5], X_train[:, 2:]\n",
    "X_valid_wide, X_valid_deep = X_valid[:, :5], X_valid[:, 2:]\n",
    "\n",
    "X_test_wide, X_test_deep = X_test[:, :5], X_test[:, 2:]\n",
    "X_new_wide, X_new_deep = X_test_wide[:3], X_test_deep[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4c64fdab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - RootMeanSquaredError: 1.1646 - loss: 1.3563 - val_RootMeanSquaredError: 0.9548 - val_loss: 0.9117\n",
      "Epoch 2/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - RootMeanSquaredError: 0.7311 - loss: 0.5346 - val_RootMeanSquaredError: 1.1819 - val_loss: 1.3970\n",
      "Epoch 3/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - RootMeanSquaredError: 0.6724 - loss: 0.4521 - val_RootMeanSquaredError: 0.9344 - val_loss: 0.8730\n",
      "Epoch 4/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 910us/step - RootMeanSquaredError: 0.6363 - loss: 0.4048 - val_RootMeanSquaredError: 1.1261 - val_loss: 1.2681\n",
      "Epoch 5/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - RootMeanSquaredError: 0.6249 - loss: 0.3906 - val_RootMeanSquaredError: 1.2635 - val_loss: 1.5963\n",
      "Epoch 6/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - RootMeanSquaredError: 0.6167 - loss: 0.3803 - val_RootMeanSquaredError: 1.5900 - val_loss: 2.5281\n",
      "Epoch 7/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - RootMeanSquaredError: 0.6072 - loss: 0.3687 - val_RootMeanSquaredError: 2.2568 - val_loss: 5.0932\n",
      "Epoch 8/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - RootMeanSquaredError: 0.6209 - loss: 0.3855 - val_RootMeanSquaredError: 2.1033 - val_loss: 4.4240\n",
      "Epoch 9/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - RootMeanSquaredError: 0.6115 - loss: 0.3739 - val_RootMeanSquaredError: 1.7064 - val_loss: 2.9118\n",
      "Epoch 10/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - RootMeanSquaredError: 0.5973 - loss: 0.3568 - val_RootMeanSquaredError: 1.8215 - val_loss: 3.3179\n",
      "Epoch 11/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - RootMeanSquaredError: 0.6071 - loss: 0.3685 - val_RootMeanSquaredError: 1.2721 - val_loss: 1.6182\n",
      "Epoch 12/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - RootMeanSquaredError: 0.5927 - loss: 0.3513 - val_RootMeanSquaredError: 1.3102 - val_loss: 1.7167\n",
      "Epoch 13/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - RootMeanSquaredError: 0.5893 - loss: 0.3472 - val_RootMeanSquaredError: 0.7526 - val_loss: 0.5664\n",
      "Epoch 14/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - RootMeanSquaredError: 0.5808 - loss: 0.3373 - val_RootMeanSquaredError: 1.1422 - val_loss: 1.3045\n",
      "Epoch 15/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - RootMeanSquaredError: 0.5814 - loss: 0.3380 - val_RootMeanSquaredError: 0.9896 - val_loss: 0.9794\n",
      "Epoch 16/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - RootMeanSquaredError: 0.5793 - loss: 0.3356 - val_RootMeanSquaredError: 1.3643 - val_loss: 1.8612\n",
      "Epoch 17/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - RootMeanSquaredError: 0.5836 - loss: 0.3406 - val_RootMeanSquaredError: 1.3325 - val_loss: 1.7756\n",
      "Epoch 18/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - RootMeanSquaredError: 0.5822 - loss: 0.3389 - val_RootMeanSquaredError: 1.2678 - val_loss: 1.6074\n",
      "Epoch 19/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - RootMeanSquaredError: 0.5768 - loss: 0.3327 - val_RootMeanSquaredError: 1.2646 - val_loss: 1.5992\n",
      "Epoch 20/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - RootMeanSquaredError: 0.5783 - loss: 0.3345 - val_RootMeanSquaredError: 1.1478 - val_loss: 1.3175\n",
      "\u001b[1m162/162\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - RootMeanSquaredError: 0.5703 - loss: 0.3253  \n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n"
     ]
    }
   ],
   "source": [
    "norm_layer_wide.adapt(X_train_wide)\n",
    "norm_layer_deep.adapt(X_train_deep)\n",
    "\n",
    "history = model.fit((X_train_wide, X_train_deep), y_train, epochs=20, validation_data=((X_valid_wide, X_valid_deep), y_valid))\n",
    "\n",
    "mse_test = model.evaluate((X_test_wide, X_test_deep), y_test)\n",
    "y_pred = model.predict((X_new_wide, X_new_deep))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f1e2ae3",
   "metadata": {},
   "source": [
    "There are also many use cases in which you may want to have multiple outputs:\n",
    "\n",
    "The task may demand it. For instance, you may want to locate and classify the main object in a picture. This is both a regression tasks and a classification task.\n",
    "\n",
    "Similarly, you may have multiple independent tasks based on the same data. Sure, you could train one neural network per task, but in many cases you will get better results on all tasks by training a single neural network with one output per task. This is because the neural network can learn features in the data that are useful across tasks. For example, you could perform multitask classification on pictures of faces, using one output to classify the person’s facial expression (smiling, surprised, etc.) and another output to identify whether they are wearing glasses or not.\n",
    "\n",
    "\n",
    "Another use case is as a regularization technique (i.e., a trainingconstraint whose objective is to reduce overfitting and thus improve the model’s ability to generalize). For example, you may want to add an auxiliary output in a neural network architecture (see Figure 10-15) to ensure that the underlying part of the network learns something useful on its own, without relying on the rest of the network.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/sambitmukherjee/handson-ml3-pytorch/main/chapter10/Figure_10-15.png\" width=\"480\">\n",
    "\n",
    "Adding an extra output is quite easy: we just connect it to the appropriate layer and add it to the model’s list of outputs. For example, the following code builds the network represented in Figure 10-15:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "92d66bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = tf.keras.layers.Dense(1)(concat)\n",
    "aux_output = tf.keras.layers.Dense(1)(hidden2)\n",
    "\n",
    "model = tf.keras.Model(inputs=[input_wide, input_deep],outputs=[output, aux_output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "149e523e",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "\n",
    "model.compile(loss=(\"mse\", \"mse\"), loss_weights=(0.9, 0.1), optimizer=optimizer, metrics=[['RootMeanSquaredError'], ['RootMeanSquaredError']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "331634fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - dense_6_RootMeanSquaredError: 1.2875 - dense_6_loss: 1.6569 - dense_7_RootMeanSquaredError: 1.1649 - dense_7_loss: 1.3565 - loss: 1.6275 - val_dense_6_RootMeanSquaredError: 0.7273 - val_dense_6_loss: 0.5288 - val_dense_7_RootMeanSquaredError: 0.7847 - val_dense_7_loss: 0.6156 - val_loss: 0.5376\n",
      "Epoch 2/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - dense_6_RootMeanSquaredError: 0.6836 - dense_6_loss: 0.4672 - dense_7_RootMeanSquaredError: 0.7465 - dense_7_loss: 0.5573 - loss: 0.4763 - val_dense_6_RootMeanSquaredError: 0.8174 - val_dense_6_loss: 0.6680 - val_dense_7_RootMeanSquaredError: 0.8215 - val_dense_7_loss: 0.6746 - val_loss: 0.6689\n",
      "Epoch 3/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - dense_6_RootMeanSquaredError: 0.6376 - dense_6_loss: 0.4066 - dense_7_RootMeanSquaredError: 0.7059 - dense_7_loss: 0.4984 - loss: 0.4157 - val_dense_6_RootMeanSquaredError: 0.6552 - val_dense_6_loss: 0.4292 - val_dense_7_RootMeanSquaredError: 0.8228 - val_dense_7_loss: 0.6767 - val_loss: 0.4541\n",
      "Epoch 4/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - dense_6_RootMeanSquaredError: 0.6120 - dense_6_loss: 0.3744 - dense_7_RootMeanSquaredError: 0.6870 - dense_7_loss: 0.4719 - loss: 0.3843 - val_dense_6_RootMeanSquaredError: 1.0512 - val_dense_6_loss: 1.1045 - val_dense_7_RootMeanSquaredError: 0.8830 - val_dense_7_loss: 0.7795 - val_loss: 1.0724\n",
      "Epoch 5/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - dense_6_RootMeanSquaredError: 0.6014 - dense_6_loss: 0.3616 - dense_7_RootMeanSquaredError: 0.6710 - dense_7_loss: 0.4502 - loss: 0.3705 - val_dense_6_RootMeanSquaredError: 0.9406 - val_dense_6_loss: 0.8844 - val_dense_7_RootMeanSquaredError: 0.9682 - val_dense_7_loss: 0.9370 - val_loss: 0.8900\n",
      "Epoch 6/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - dense_6_RootMeanSquaredError: 0.5934 - dense_6_loss: 0.3521 - dense_7_RootMeanSquaredError: 0.6645 - dense_7_loss: 0.4414 - loss: 0.3611 - val_dense_6_RootMeanSquaredError: 0.8944 - val_dense_6_loss: 0.7996 - val_dense_7_RootMeanSquaredError: 0.7638 - val_dense_7_loss: 0.5833 - val_loss: 0.7782\n",
      "Epoch 7/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - dense_6_RootMeanSquaredError: 0.5851 - dense_6_loss: 0.3424 - dense_7_RootMeanSquaredError: 0.6514 - dense_7_loss: 0.4244 - loss: 0.3505 - val_dense_6_RootMeanSquaredError: 0.5878 - val_dense_6_loss: 0.3454 - val_dense_7_RootMeanSquaredError: 0.6413 - val_dense_7_loss: 0.4113 - val_loss: 0.3520\n",
      "Epoch 8/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - dense_6_RootMeanSquaredError: 0.5791 - dense_6_loss: 0.3354 - dense_7_RootMeanSquaredError: 0.6443 - dense_7_loss: 0.4152 - loss: 0.3433 - val_dense_6_RootMeanSquaredError: 0.6139 - val_dense_6_loss: 0.3768 - val_dense_7_RootMeanSquaredError: 0.7078 - val_dense_7_loss: 0.5008 - val_loss: 0.3893\n",
      "Epoch 9/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - dense_6_RootMeanSquaredError: 0.5788 - dense_6_loss: 0.3350 - dense_7_RootMeanSquaredError: 0.6392 - dense_7_loss: 0.4086 - loss: 0.3424 - val_dense_6_RootMeanSquaredError: 0.6986 - val_dense_6_loss: 0.4878 - val_dense_7_RootMeanSquaredError: 0.6760 - val_dense_7_loss: 0.4569 - val_loss: 0.4849\n",
      "Epoch 10/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - dense_6_RootMeanSquaredError: 0.5760 - dense_6_loss: 0.3318 - dense_7_RootMeanSquaredError: 0.6340 - dense_7_loss: 0.4020 - loss: 0.3388 - val_dense_6_RootMeanSquaredError: 0.8584 - val_dense_6_loss: 0.7365 - val_dense_7_RootMeanSquaredError: 1.0523 - val_dense_7_loss: 1.1069 - val_loss: 0.7739\n",
      "Epoch 11/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - dense_6_RootMeanSquaredError: 0.5733 - dense_6_loss: 0.3287 - dense_7_RootMeanSquaredError: 0.6314 - dense_7_loss: 0.3987 - loss: 0.3357 - val_dense_6_RootMeanSquaredError: 0.7327 - val_dense_6_loss: 0.5367 - val_dense_7_RootMeanSquaredError: 0.7304 - val_dense_7_loss: 0.5333 - val_loss: 0.5365\n",
      "Epoch 12/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - dense_6_RootMeanSquaredError: 0.5725 - dense_6_loss: 0.3277 - dense_7_RootMeanSquaredError: 0.6281 - dense_7_loss: 0.3944 - loss: 0.3344 - val_dense_6_RootMeanSquaredError: 1.1201 - val_dense_6_loss: 1.2540 - val_dense_7_RootMeanSquaredError: 1.2121 - val_dense_7_loss: 1.4686 - val_loss: 1.2760\n",
      "Epoch 13/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - dense_6_RootMeanSquaredError: 0.5763 - dense_6_loss: 0.3321 - dense_7_RootMeanSquaredError: 0.6312 - dense_7_loss: 0.3984 - loss: 0.3387 - val_dense_6_RootMeanSquaredError: 0.9589 - val_dense_6_loss: 0.9191 - val_dense_7_RootMeanSquaredError: 1.0851 - val_dense_7_loss: 1.1770 - val_loss: 0.9453\n",
      "Epoch 14/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - dense_6_RootMeanSquaredError: 0.5716 - dense_6_loss: 0.3267 - dense_7_RootMeanSquaredError: 0.6259 - dense_7_loss: 0.3916 - loss: 0.3332 - val_dense_6_RootMeanSquaredError: 0.9083 - val_dense_6_loss: 0.8247 - val_dense_7_RootMeanSquaredError: 0.8474 - val_dense_7_loss: 0.7179 - val_loss: 0.8144\n",
      "Epoch 15/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - dense_6_RootMeanSquaredError: 0.5702 - dense_6_loss: 0.3251 - dense_7_RootMeanSquaredError: 0.6216 - dense_7_loss: 0.3864 - loss: 0.3313 - val_dense_6_RootMeanSquaredError: 0.6545 - val_dense_6_loss: 0.4282 - val_dense_7_RootMeanSquaredError: 0.7656 - val_dense_7_loss: 0.5860 - val_loss: 0.4441\n",
      "Epoch 16/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - dense_6_RootMeanSquaredError: 0.5656 - dense_6_loss: 0.3198 - dense_7_RootMeanSquaredError: 0.6176 - dense_7_loss: 0.3814 - loss: 0.3261 - val_dense_6_RootMeanSquaredError: 0.7028 - val_dense_6_loss: 0.4938 - val_dense_7_RootMeanSquaredError: 0.6905 - val_dense_7_loss: 0.4768 - val_loss: 0.4922\n",
      "Epoch 17/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - dense_6_RootMeanSquaredError: 0.5670 - dense_6_loss: 0.3214 - dense_7_RootMeanSquaredError: 0.6164 - dense_7_loss: 0.3798 - loss: 0.3273 - val_dense_6_RootMeanSquaredError: 0.7424 - val_dense_6_loss: 0.5510 - val_dense_7_RootMeanSquaredError: 0.7884 - val_dense_7_loss: 0.6214 - val_loss: 0.5582\n",
      "Epoch 18/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - dense_6_RootMeanSquaredError: 0.5651 - dense_6_loss: 0.3193 - dense_7_RootMeanSquaredError: 0.6138 - dense_7_loss: 0.3768 - loss: 0.3251 - val_dense_6_RootMeanSquaredError: 0.7021 - val_dense_6_loss: 0.4928 - val_dense_7_RootMeanSquaredError: 0.7234 - val_dense_7_loss: 0.5232 - val_loss: 0.4960\n",
      "Epoch 19/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - dense_6_RootMeanSquaredError: 0.5637 - dense_6_loss: 0.3178 - dense_7_RootMeanSquaredError: 0.6121 - dense_7_loss: 0.3747 - loss: 0.3234 - val_dense_6_RootMeanSquaredError: 0.5901 - val_dense_6_loss: 0.3481 - val_dense_7_RootMeanSquaredError: 0.6231 - val_dense_7_loss: 0.3882 - val_loss: 0.3522\n",
      "Epoch 20/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - dense_6_RootMeanSquaredError: 0.5601 - dense_6_loss: 0.3137 - dense_7_RootMeanSquaredError: 0.6084 - dense_7_loss: 0.3702 - loss: 0.3194 - val_dense_6_RootMeanSquaredError: 0.5578 - val_dense_6_loss: 0.3111 - val_dense_7_RootMeanSquaredError: 0.6576 - val_dense_7_loss: 0.4324 - val_loss: 0.3233\n"
     ]
    }
   ],
   "source": [
    "norm_layer_wide.adapt(X_train_wide)\n",
    "norm_layer_deep.adapt(X_train_deep)\n",
    "\n",
    "history = model.fit(\n",
    "    (X_train_wide, X_train_deep), (y_train, y_train), epochs=20, \n",
    "    validation_data=((X_valid_wide, X_valid_deep), (y_valid, y_valid))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3b48e5ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m162/162\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - dense_6_RootMeanSquaredError: 0.5646 - dense_6_loss: 0.3184 - dense_7_RootMeanSquaredError: 0.6106 - dense_7_loss: 0.3729 - loss: 0.3242 \n"
     ]
    }
   ],
   "source": [
    "eval_results = model.evaluate((X_test_wide, X_test_deep), (y_test, y_test))\n",
    "weighted_sum_of_losses, main_loss, aux_loss, main_rmse, aux_rmse = eval_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d04a9680",
   "metadata": {},
   "source": [
    "If you set return_dict=True, then evaluate() will return a dictionary instead of a big tuple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b2928e7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n"
     ]
    }
   ],
   "source": [
    "y_pred_main, y_pred_aux = model.predict((X_new_wide, X_new_deep))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "df64c8eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n"
     ]
    }
   ],
   "source": [
    "y_pred_tuple = model.predict((X_new_wide, X_new_deep))\n",
    "y_pred = dict(zip(model.output_names, y_pred_tuple))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e8fd2be",
   "metadata": {},
   "source": [
    "# Using the Subclassing API to Build Dynamic Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef45a89",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.13.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
